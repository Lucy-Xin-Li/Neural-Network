{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Multilayer Perceptron \n",
    "\n",
    "## Context\n",
    "You are given the task of writing a multi-layer perceptron that determines whether user interactions with a website leads to a purchase (class 1) or not (class 0). Other data scientists have already collected and quantized the interactions into a vector of 100 numbers, which will serve as your inputs to your model. You are to predict whether future interactions will result in a purchase or not.\n",
    "\n",
    "## Procedure\n",
    "In this assignment you'll use Keras to write and tune a multi-layer perceptron. You are provided with a training dataset, and your model will be evaluated on a testing set that you won't have access to. \n",
    "\n",
    "The architecture of the model and how you fit it is up to you. However, you are not to use any recurrent or convolutional layers yet. Write at least two models and compare them using cross validation (See the cross validation notebook for reference). For your own sanity, just use K<=4 for KFold if you go that route. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# feel free to add more imports here\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as ms\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD  # use SGD in your model\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load data\n",
    "\n",
    "Load in the data below-- set the path correctly so that you can read in \"data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "path_to_data = \"/tmp/data.csv\"  # modify this\n",
    "######################\n",
    "\n",
    "# load data\n",
    "ds = np.loadtxt(path_to_data, delimiter=\",\")\n",
    "y_train = ds[:, 0]  # first column is the labels\n",
    "X_train = ds[:, 1:]  # rest of the columns are the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already convert y to categorical\n",
    "y_train_vectorized = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to specify the architecture of your model in a list to stay organized. Modify \"layer_sizes\" below to an architecture of your choosing. Note that the first and last elements are already included for you, so please enter your hidden layers inbetween."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Specify model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# example:\n",
    "# layer_sizes = [X_train.shape[1], 5, y_train_vectorized.shape[1]]  \n",
    "# remember the first and last layers need to have the same dimensionality as your input and output\n",
    "model1_layer_sizes = []\n",
    "model2_layer_sizes = []\n",
    "# feel free to add more models if you want to explore\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build models\n",
    "Define build_model() functions below to construct your models. You can use the below cell as a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model1():  # make sure you change the function name for each model!\n",
    "    model = Sequential()\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    # build your model. remember the input to the first layer needs to be layer_sizes[0]\n",
    "    model.add(Dense(input_dim=model1_layer_sizes[0],\n",
    "                    units=model1_layer_sizes[1],\n",
    "                    kernel_initializer=\"uniform\",\n",
    "                    activation=\"relu\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    ######################\n",
    "    # we write the last layer for you.\n",
    "    # Finally, add a readout layer, mapping to output units using the softmax function\n",
    "    model.add(Dense(units=model1_layer_sizes[-1], # last layer\n",
    "                    kernel_initializer='uniform',\n",
    "                    activation=\"softmax\"))\n",
    "    \n",
    "    sgd = SGD(lr=0.001, decay=1e-7, momentum=.9)  # Stochastic gradient descent, leave these parameters fixed\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])  \n",
    "    # we'll have the categorical crossentropy as the loss function\n",
    "    # we also want the model to automatically calculate accuracy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2():  # make sure you change the function name for each model!\n",
    "    model = Sequential()\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    # build your model. remember the input to the first layer needs to be layer_sizes[0]\n",
    "    model.add(Dense(input_dim=model2_layer_sizes[0],\n",
    "                    units=model2_layer_sizes[1],\n",
    "                    kernel_initializer=\"uniform\",\n",
    "                    activation=\"relu\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    ######################\n",
    "    # we write the last layer for you.\n",
    "    # Finally, add a readout layer, mapping to output units using the softmax function\n",
    "    model.add(Dense(units=model2_layer_sizes[-1], # last layer\n",
    "                    kernel_initializer='uniform',\n",
    "                    activation=\"softmax\"))\n",
    "    \n",
    "    sgd = SGD(lr=0.001, decay=1e-7, momentum=.9)  # Stochastic gradient descent, leave these parameters fixed\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])  \n",
    "    # we'll have the categorical crossentropy as the loss function\n",
    "    # we also want the model to automatically calculate accuracy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Cross validate and train models\n",
    "\n",
    "You will write the entire cross-validation process for all of your proposed models. Print the average accuracies so you can compare them.\n",
    "\n",
    "For each .fit() function, use epochs=500, batch_size=50, verbose=0 (as in the keras-cv notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kfold here:\n",
    "kf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation for model 1 (refer to keras-cv)\n",
    "# we can pass any dataset into kf.split(), and it will return the indices of the \"train\" and \"validation\" partitions\n",
    "accuracies = []\n",
    "\n",
    "# STEP 1: partition the data chunks and iterate through them\n",
    "# write the for loop here\n",
    "\n",
    "    \n",
    "    # build your model below\n",
    "    model1 = build_model1()\n",
    "    \n",
    "    # STEP 2: train the model on the k-1 chunks using the options above    \n",
    "    model1.fit()  # fill in the arguments for .fit()\n",
    "    \n",
    "    # STEP 3: predict the kth chunk and evaluate accuracy \n",
    "    # this is implemented for you\n",
    "    proba = model1.predict_proba(X_train[val_idx], batch_size=32)  # predict the classes for the validation set\n",
    "    classes = np.argmax(proba, axis=1)\n",
    "    \n",
    "    # save the accuracy (implemented for you)\n",
    "    accuracies.append(accuracy_score(y_train[val_idx], classes))\n",
    "\n",
    "# STEP 4: average across the k accuracies\n",
    "model1_accuracy = np.array(accuracies).mean()  # the mean performance of model 1\n",
    "print(model1_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation for model 2 (refer to keras-cv)\n",
    "accuracies = []\n",
    "\n",
    "# STEP 1: partition the data chunks and iterate through them\n",
    "\n",
    "\n",
    "    # STEP 2: train the model on the k-1 chunks using the options above    \n",
    "    model1.fit()  # fill in the arguments for .fit()\n",
    "    \n",
    "    # STEP 3: predict the kth chunk and evaluate accuracy \n",
    "    # this is implemented for you\n",
    "    proba = model1.predict_proba(X_train[val_idx], batch_size=32)  # predict the classes for the validation set\n",
    "    classes = np.argmax(proba, axis=1)\n",
    "    \n",
    "    # save the accuracy (implemented for you)\n",
    "    accuracies.append(accuracy_score(y_train[val_idx], classes))\n",
    "\n",
    "# STEP 4: average across the k accuracies\n",
    "model2_accuracy = np.array(accuracies).mean()  # the mean performance of model 1\n",
    "print(model2_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation for other models if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Select and train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model =  # use the build function for the model you selected\n",
    "\n",
    "final_model.fit(X_train, y_train_vectorized, \n",
    "                epochs=1000, batch_size=50, verbose = 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Export (save) the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name =  # set the final file name here\n",
    "final_model.save(output_name)  # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
