{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross Validation with Keras\n",
    "\n",
    "A brief guide to k-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is the current gold standard to preventing overfitting. The essence of all cross validation methods are the same: We split our data into one or more \"train\" sets, where we conduct all model fitting and parameter tuning, and a \"validation\" set where the model is evaluated. \n",
    "\n",
    "Typically, cross-validation is used to select the best free parameters of a model in order to train a final model on the entire dataset. \n",
    "\n",
    "In the context of neural networks, the \"free parameters\" refer to the architecture of the network (i.e. how many layers, how many neurons in each layer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.datasets import make_classification\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some data and split it to a training set and testing set. All of our fitting and optimization will be conducted on the training set and NOT the testing set. To emphasize this point, any data the model is tested on for validation is referred to as a \"validation\" set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=200, n_features=10, \n",
    "                                     n_informative=5, \n",
    "                                     scale=2.0, \n",
    "                                     shuffle=True, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "# put X_test and y_test in a \"box\" for later. We won't touch these until the very end.\n",
    "\n",
    "# We also need to vectorize y_train for neural network testing (See other notebook for an explanation)\n",
    "y_train_vectorized = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While scikit-learn's cross-validation utilities are designed to be used with their classifiers, they are general purpose and can interface with keras nicely.\n",
    "\n",
    "## Set up models\n",
    "Let's create our candidate models that we would like to compare using Keras (refer to the other notebook for an explanation of all of the steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.001, decay=1e-7, momentum=.9)  # Stochastic gradient descent\n",
    "model1_layer_sizes = [X_train.shape[1], 10, 5, np.unique(y_train).shape[0]]  \n",
    "# 2 hidden layers of size 10 and 5, respectively\n",
    "model2_layer_sizes = [X_train.shape[1], 5, 5, 10, 10, 5, 5, np.unique(y_train).shape[0]]  \n",
    "# 6 hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model 1\n",
    "def build_model1():\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(\n",
    "            input_dim=model1_layer_sizes[0], \n",
    "            units=model1_layer_sizes[1],\n",
    "            activation=\"relu\"\n",
    "        ))\n",
    "\n",
    "    # Now our second hidden layer with 10 inputs (from the first\n",
    "    # hidden layer) and 5 outputs. Also with ReLU activation\n",
    "    model1.add(Dense(\n",
    "        input_dim=model1_layer_sizes[1], \n",
    "        units=model1_layer_sizes[2],\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "\n",
    "    # Finally, add a readout layer, mapping the 5 hidden units\n",
    "    # to two output units using the softmax function\n",
    "    model1.add(Dense(units=model1_layer_sizes[3], \n",
    "                    kernel_initializer='uniform',\n",
    "                    activation=\"softmax\"))\n",
    "\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])  \n",
    "\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model 2\n",
    "def build_model2():\n",
    "\n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(\n",
    "            input_dim=model2_layer_sizes[0], \n",
    "            units=model2_layer_sizes[1],\n",
    "            activation=\"relu\"\n",
    "        ))\n",
    "\n",
    "    # Now our second hidden layer with 10 inputs (from the first\n",
    "    # hidden layer) and 5 outputs. Also with ReLU activation\n",
    "    model2.add(Dense(\n",
    "        input_dim=model2_layer_sizes[1], \n",
    "        units=model2_layer_sizes[2],\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "\n",
    "    model2.add(Dense(\n",
    "        input_dim=model2_layer_sizes[2], \n",
    "        units=model2_layer_sizes[3],\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "\n",
    "    model2.add(Dense(\n",
    "        input_dim=model2_layer_sizes[3], \n",
    "        units=model2_layer_sizes[4],\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "\n",
    "    model2.add(Dense(\n",
    "        input_dim=model2_layer_sizes[4], \n",
    "        units=model2_layer_sizes[5],\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "\n",
    "    model2.add(Dense(\n",
    "        input_dim=model2_layer_sizes[5], \n",
    "        units=model2_layer_sizes[6],\n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "\n",
    "    # Finally, add a readout layer, mapping the 5 hidden units\n",
    "    # to two output units using the softmax function\n",
    "    model2.add(Dense(units=model2_layer_sizes[7], \n",
    "                    kernel_initializer='uniform',\n",
    "                    activation=\"softmax\"))\n",
    "\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])  \n",
    "\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up K-fold validation\n",
    "\n",
    "Recall that in K-fold validation, the data are partitioned into k chunks so that every data point serves as a testing point exactly once. A sketch of the k-fold procedure is written below:\n",
    "\n",
    "1) partition the data into k chunks. \n",
    "\n",
    "2) train your model using k-1 chunks\n",
    "\n",
    "3) test your model on the remaining chunk\n",
    "\n",
    "4) repeat for all k chunks and average the results\n",
    "\n",
    "The partitioning procedure is already implemented in scikit learn. We demonstrate its use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4  # 4 fold cross validation\n",
    "kf = ms.KFold(k, shuffle=True)  # kf is an object that gets applied to the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can pass any dataset into kf.split(), and it will return the indices of the \"train\" and \"validation\" partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 25\n",
      "75 25\n",
      "75 25\n",
      "75 25\n",
      "[ 0  1  3  5  6  7  8  9 10 11 12 14 15 16 17 18 19 21 22 23 24 26 27 28\n",
      " 29 33 34 35 36 39 41 42 43 45 46 47 48 49 50 53 54 55 56 57 59 60 61 62\n",
      " 63 64 65 66 68 70 71 72 76 78 79 81 82 83 85 86 88 89 90 91 92 93 94 96\n",
      " 97 98 99] [ 2  4 13 20 25 30 31 32 37 38 40 44 51 52 58 67 69 73 74 75 77 80 84 87\n",
      " 95]\n"
     ]
    }
   ],
   "source": [
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    # train_idx are the indices of the training set\n",
    "    # val_idx are the indices of the validation set\n",
    "    # notice we haven't used the testing set (from the beginning of this notebook!)\n",
    "    print(len(train_idx), len(val_idx))\n",
    "    \n",
    "print(train_idx, val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sense of that output above:\n",
    "\n",
    "The first 4 iterations are just the size of the training and validation sets. as you can see, every iteration of the k-fold the training set is of size 75, and the validation set is of size 25.\n",
    "\n",
    "Finally, we print the indices from the last iteration just to verify that the splitting worked.\n",
    "\n",
    "Let's use the kf object to conduct cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation workflow:\n",
    "\n",
    "We will use K-fold cross-validation to determine which of two multi-layer perceptron architectures is best for the data we have. The proper procedure for selecting and t0raining a model is as follows:\n",
    "\n",
    "1. Use cross-validation on the training set to determine the correct free parameters\n",
    "\n",
    "2. Train the model on the entire dataset using the optimized parameters\n",
    "\n",
    "3. Test the model on the testing set (aka deploy the model)\n",
    "\n",
    "### Step 1: Run cross validation and compare the average performance for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation for model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can pass any dataset into kf.split(), and it will return the indices of the \"train\" and \"validation\" partitions\n",
    "accuracies = []\n",
    "\n",
    "# STEP 1: partition the data chunks\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    # train_idx are the indices of the training set\n",
    "    # val_idx are the indices of the validation set\n",
    "    \n",
    "    # we need to rebuild the model every time because by default it learns incrementally\n",
    "    model1 = build_model1()  \n",
    "    \n",
    "    # STEP 2: train the model on the k-1 chunks\n",
    "    model1.fit(X_train[train_idx], y_train_vectorized[train_idx], epochs=500, batch_size=50, verbose = 0)\n",
    "    \n",
    "    # STEP 3: predict the kth chunk and evaluate accuracy\n",
    "    proba = model1.predict_proba(X_train[val_idx], batch_size=32)  # predict the classes for the validation set\n",
    "    classes = np.argmax(proba, axis=1)\n",
    "    \n",
    "    # save the accuracy\n",
    "    accuracies.append(accuracy_score(y_train[val_idx], classes))\n",
    "\n",
    "# STEP 4: average across the k accuracies\n",
    "model1_accuracy = np.array(accuracies).mean()  # the mean performance of model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can pass any dataset into kf.split(), and it will return the indices of the \"train\" and \"validation\" partitions\n",
    "accuracies = []\n",
    "\n",
    "# STEP 1: partition the data chunks\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    # train_idx are the indices of the training set\n",
    "    # val_idx are the indices of the validation set\n",
    "    \n",
    "    # we need to rebuild the model every time because by default it learns incrementally\n",
    "    model2 = build_model1()  \n",
    "    \n",
    "    # STEP 2: train the model on the k-1 chunks\n",
    "    model2.fit(X_train[train_idx], y_train_vectorized[train_idx], epochs=500, batch_size=50, verbose = 0)\n",
    "    \n",
    "    # STEP 3: predict the kth chunk and evaluate accuracy\n",
    "    proba = model2.predict_proba(X_train[val_idx], batch_size=32)  # predict the classes for the validation set\n",
    "    classes = np.argmax(proba, axis=1)\n",
    "    \n",
    "    # save the accuracy\n",
    "    accuracies.append(accuracy_score(y_train[val_idx], classes))\n",
    "\n",
    "# STEP 4: average across the k accuracies\n",
    "model2_accuracy = np.array(accuracies).mean()  # the mean performance of model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two averaged accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: 0.76 | Model 2: 0.7100000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 1: {} | Model 2: {}\".format(model1_accuracy, model2_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that after 1000 epochs, model 1 seems to perform better than model 2. So, let's select model1 as our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fit the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1326703c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = build_model1()\n",
    "final_model.fit(X_train, y_train_vectorized, \n",
    "                epochs=500, batch_size=50, verbose = 0)  \n",
    "# fit the final model on the FULL training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the final model\n",
    "\n",
    "Notice that we haven't touched X_test and y_test until now, and we ONLY use them once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set performance: 0.77\n"
     ]
    }
   ],
   "source": [
    "proba = final_model.predict_proba(X_test, batch_size=32)  # predict the classes for the validation set\n",
    "classes = np.argmax(proba, axis=1)\n",
    "print(\"Test set performance: {}\".format(accuracy_score(y_test, classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we wouldn't fit the second model because we already chose the first. But, we fit the second model below just to show that indeed, cross validation guided us towards the correct model.\n",
    "\n",
    "(Note that by chance, sometimes model 2 will actually perform better than model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set performance: 0.48\n"
     ]
    }
   ],
   "source": [
    "final_model_alt = build_model2()\n",
    "final_model_alt.fit(X_train, y_train_vectorized, epochs=500, batch_size=50, verbose = 0)\n",
    "proba = final_model_alt.predict_proba(X_test, batch_size=32)  # predict the classes for the validation set\n",
    "classes = np.argmax(proba, axis=1)\n",
    "print(\"Test set performance: {}\".format(accuracy_score(y_test, classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats\n",
    "\n",
    "In the context of deep learning, the number of epochs should always be set as high as possible (keeping computation time in mind). This will be discussed later when we cover stochastic gradient descent. The problem of \"overfitting\" arises from the number of parameters being fitted, which manifests as the architecture of the model, rather than the number of iterations through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
